{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "948fc274",
   "metadata": {},
   "source": [
    "# Module 1, Week 1, Assignment 1: Tokenization and Preprocessing\n",
    "\n",
    "In this assignment, you will explore the fundamentals of text preprocessing, a critical step in any NLP workflow. Follow the instructions provided in the notebook and complete the activities to solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bee2f4",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Understand the importance of text preprocessing in NLP.\n",
    "- Learn to tokenize text into sentences and words.\n",
    "- Remove punctuation and special characters from text.\n",
    "- Perform basic normalization by converting text to lowercase.\n",
    "- Use NLTK to remove stopwords from the text.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "1. Run the provided code cells to see examples of preprocessing techniques.\n",
    "2. Complete the **TODO** sections to practice your skills.\n",
    "3. Analyze and compare the raw vs. preprocessed text in the final activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51db24",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "First, we will import the libraries needed for tokenization and text preprocessing. NLTK will be the primary library for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')  # For stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097beb1",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization\n",
    "Tokenization is the process of breaking down text into smaller units, such as words or sentences. Let's practice tokenizing a sample paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94495a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Text\n",
    "text = \"Natural Language Processing (NLP) is a fascinating field of artificial intelligence. It focuses on the interaction between computers and human language. Tokenization is a crucial step in NLP pipelines!\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\\n\", sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"\\nWord Tokenization:\\n\", words)\n",
    "\n",
    "# TODO: Tokenize another sample paragraph of your choice\n",
    "# Add your own text below and apply sentence and word tokenization\n",
    "# text_custom = \"Your text here\"\n",
    "# sentences_custom = sent_tokenize(text_custom)\n",
    "# words_custom = word_tokenize(text_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3a099",
   "metadata": {},
   "source": [
    "## Step 3: Removing Punctuation and Special Characters\n",
    "Punctuation and special characters often don't carry meaningful information for NLP tasks. Removing them can help simplify the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a19bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "text_no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Text without punctuation:\\n\", text_no_punct)\n",
    "\n",
    "# TODO: Remove punctuation from your custom text (from Step 2)\n",
    "# text_custom_no_punct = text_custom.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5979a8",
   "metadata": {},
   "source": [
    "## Step 4: Lowercasing and Normalization\n",
    "Normalization involves converting text to a consistent format, such as lowercasing, to reduce variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Text to Lowercase\n",
    "text_lower = text_no_punct.lower()\n",
    "print(\"Lowercased Text:\\n\", text_lower)\n",
    "\n",
    "# TODO: Apply lowercasing to your custom text (from Step 3)\n",
    "# text_custom_lower = text_custom_no_punct.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5205e4",
   "metadata": {},
   "source": [
    "## Step 5: Removing Stopwords\n",
    "Stopwords are common words (e.g., \"and\", \"the\", \"is\") that often do not add much meaning to a sentence. Removing stopwords can help focus on the more meaningful words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a277ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Stopword List\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove Stopwords\n",
    "words_no_stopwords = [word for word in word_tokenize(text_lower) if word not in stop_words]\n",
    "print(\"Text without stopwords:\\n\", words_no_stopwords)\n",
    "\n",
    "# TODO: Remove stopwords from your custom text (from Step 4)\n",
    "# words_custom_no_stopwords = [word for word in word_tokenize(text_custom_lower) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f2de7",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Raw vs. Preprocessed Text\n",
    "Compare the original text with the preprocessed version to understand the impact of these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb06cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Text:\\n\", text)\n",
    "print(\"\\nPreprocessed Text:\\n\", ' '.join(words_no_stopwords))\n",
    "\n",
    "# TODO: Print the original and preprocessed versions of your custom text\n",
    "# print(\"Custom Original Text:\\n\", text_custom)\n",
    "# print(\"\\nCustom Preprocessed Text:\\n\", ' '.join(words_custom_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb928d",
   "metadata": {},
   "source": [
    "### Congratulations! ðŸŽ‰\n",
    "You have completed the tokenization and preprocessing assignment. These preprocessing steps are foundational in NLP workflows and will help you tackle more advanced topics in the future.\n",
    "\n",
    "---\n",
    "\n",
    "## Reflection:\n",
    "- How did removing punctuation and stopwords change the text?\n",
    "- Were there any cases where removing stopwords might not have been ideal?\n",
    "\n",
    "Feel free to experiment with more text samples and explore additional preprocessing techniques!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
