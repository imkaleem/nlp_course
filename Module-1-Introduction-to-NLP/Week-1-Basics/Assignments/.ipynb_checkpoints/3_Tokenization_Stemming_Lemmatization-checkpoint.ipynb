{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Week 2: Assignment - Stemming, Lemmatization, and Advanced Tokenization\n",
    "\n",
    "### Objective\n",
    "This assignment is designed to:\n",
    "1. Introduce you to stemming and lemmatization.\n",
    "2. Teach you advanced tokenization techniques using NLTK and SpaCy.\n",
    "3. Help you compare different preprocessing techniques and their impact on text data.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "- Use the provided text or your own sample text for analysis.\n",
    "- Perform stemming, lemmatization, and advanced tokenization on the text.\n",
    "- Analyze the results and reflect on the differences between these techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Download Required NLTK Data Files\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load SpaCy Model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Input Text\n",
    "You can use the sample text below or replace it with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Raw Text\n",
    "raw_text = \"\"\"\n",
    "Text preprocessing is a critical step in natural language processing tasks. \n",
    "It involves converting text into a format that is easy for machines to understand. \n",
    "Key steps include stemming, lemmatization, and tokenization.\n",
    "\"\"\"\n",
    "\n",
    "# Print Raw Text\n",
    "print(\"Raw Text:\\n\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization with NLTK\n",
    "Tokenize the text into words using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text Using NLTK\n",
    "nltk_tokens = word_tokenize(raw_text)\n",
    "print(\"\\nNLTK Tokenized Words:\\n\")\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Stemming\n",
    "Perform stemming using NLTK's PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply Stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in nltk_tokens]\n",
    "print(\"\\nStemmed Words:\\n\")\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Lemmatization\n",
    "Perform lemmatization using NLTK's WordNetLemmatizer and SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply Lemmatization with NLTK\n",
    "nltk_lemmatized = [lemmatizer.lemmatize(word) for word in nltk_tokens]\n",
    "print(\"\\nNLTK Lemmatized Words:\\n\")\n",
    "print(nltk_lemmatized)\n",
    "\n",
    "# Apply Lemmatization with SpaCy\n",
    "doc = nlp(raw_text)\n",
    "spacy_lemmatized = [token.lemma_ for token in doc]\n",
    "print(\"\\nSpaCy Lemmatized Words:\\n\")\n",
    "print(spacy_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Tokenization with SpaCy\n",
    "Perform advanced tokenization using SpaCy, which includes handling punctuation and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tokenization with SpaCy\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"\\nAdvanced Tokenization with SpaCy:\\n\")\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Results\n",
    "Analyze and compare the output of stemming, lemmatization, and tokenization techniques. Discuss the differences and their implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Results\n",
    "print(\"\\nComparison of Techniques:\\n\")\n",
    "print(\"Raw Text:\", raw_text)\n",
    "print(\"\\nStemmed Words:\", stemmed_words)\n",
    "print(\"\\nNLTK Lemmatized Words:\", nltk_lemmatized)\n",
    "print(\"\\nSpaCy Lemmatized Words:\", spacy_lemmatized)\n",
    "print(\"\\nNLTK Tokenized Words:\", nltk_tokens)\n",
    "print(\"\\nSpaCy Tokenized Words:\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. What differences did you observe between stemming and lemmatization?\n",
    "2. Which tokenization technique (NLTK vs. SpaCy) provided better results for your text?\n",
    "3. How might the choice of preprocessing technique impact downstream NLP tasks like classification or summarization?\n",
    "4. Experiment with different texts. How do the results vary for complex sentences or domain-specific text?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}