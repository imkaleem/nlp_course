{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Week 1: Assignment - Stemming, Lemmatization, and Advanced Tokenization\n",
    "\n",
    "### Objective\n",
    "This assignment is designed to:\n",
    "1. Introduce to stemming and lemmatization.\n",
    "2. Teach advanced tokenization techniques using NLTK and SpaCy.\n",
    "3. Help compare different preprocessing techniques and their impact on text data.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "- Use the provided text or your own sample text for analysis.\n",
    "- Perform stemming, lemmatization, and advanced tokenization on the text.\n",
    "- Analyze the results and reflect on the differences between these techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaleem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaleem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kaleem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Download Required NLTK Data Files\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load SpaCy Model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Input Text\n",
    "You can use the sample text below or replace it with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text:\n",
      "\n",
      "\n",
      "Text preprocessing is a critical step in natural language processing tasks. \n",
      "It involves converting text into a format that is easy for machines to understand. \n",
      "Key steps include stemming, lemmatization, and tokenization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Raw Text\n",
    "raw_text = \"\"\"\n",
    "Text preprocessing is a critical step in natural language processing tasks. \n",
    "It involves converting text into a format that is easy for machines to understand. \n",
    "Key steps include stemming, lemmatization, and tokenization.\n",
    "\"\"\"\n",
    "\n",
    "# Print Raw Text\n",
    "print(\"Raw Text:\\n\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization with NLTK\n",
    "Tokenize the text into words using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK Tokenized Words:\n",
      "\n",
      "['Text', 'preprocessing', 'is', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'tasks', '.', 'It', 'involves', 'converting', 'text', 'into', 'a', 'format', 'that', 'is', 'easy', 'for', 'machines', 'to', 'understand', '.', 'Key', 'steps', 'include', 'stemming', ',', 'lemmatization', ',', 'and', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Text Using NLTK\n",
    "nltk_tokens = word_tokenize(raw_text)\n",
    "print(\"\\nNLTK Tokenized Words:\\n\")\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Stemming\n",
    "Perform stemming using NLTK's PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Words:\n",
      "\n",
      "['text', 'preprocess', 'is', 'a', 'critic', 'step', 'in', 'natur', 'languag', 'process', 'task', '.', 'it', 'involv', 'convert', 'text', 'into', 'a', 'format', 'that', 'is', 'easi', 'for', 'machin', 'to', 'understand', '.', 'key', 'step', 'includ', 'stem', ',', 'lemmat', ',', 'and', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply Stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in nltk_tokens]\n",
    "print(\"\\nStemmed Words:\\n\")\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Lemmatization\n",
    "Perform lemmatization using NLTK's WordNetLemmatizer and SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK Lemmatized Words:\n",
      "\n",
      "['Text', 'preprocessing', 'is', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'task', '.', 'It', 'involves', 'converting', 'text', 'into', 'a', 'format', 'that', 'is', 'easy', 'for', 'machine', 'to', 'understand', '.', 'Key', 'step', 'include', 'stemming', ',', 'lemmatization', ',', 'and', 'tokenization', '.']\n",
      "\n",
      "SpaCy Lemmatized Words:\n",
      "\n",
      "['\\n', 'text', 'preprocessing', 'be', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'task', '.', '\\n', 'it', 'involve', 'convert', 'text', 'into', 'a', 'format', 'that', 'be', 'easy', 'for', 'machine', 'to', 'understand', '.', '\\n', 'key', 'step', 'include', 'stem', ',', 'lemmatization', ',', 'and', 'tokenization', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply Lemmatization with NLTK\n",
    "nltk_lemmatized = [lemmatizer.lemmatize(word) for word in nltk_tokens]\n",
    "print(\"\\nNLTK Lemmatized Words:\\n\")\n",
    "print(nltk_lemmatized)\n",
    "\n",
    "# Apply Lemmatization with SpaCy\n",
    "doc = nlp(raw_text)\n",
    "spacy_lemmatized = [token.lemma_ for token in doc]\n",
    "print(\"\\nSpaCy Lemmatized Words:\\n\")\n",
    "print(spacy_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Tokenization with SpaCy\n",
    "Perform advanced tokenization using SpaCy, which includes handling punctuation and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced Tokenization with SpaCy:\n",
      "\n",
      "['\\n', 'Text', 'preprocessing', 'is', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'tasks', '.', '\\n', 'It', 'involves', 'converting', 'text', 'into', 'a', 'format', 'that', 'is', 'easy', 'for', 'machines', 'to', 'understand', '.', '\\n', 'Key', 'steps', 'include', 'stemming', ',', 'lemmatization', ',', 'and', 'tokenization', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Advanced Tokenization with SpaCy\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"\\nAdvanced Tokenization with SpaCy:\\n\")\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Results\n",
    "Analyze and compare the output of stemming, lemmatization, and tokenization techniques. Discuss the differences and their implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Techniques:\n",
      "\n",
      "Raw Text: \n",
      "Text preprocessing is a critical step in natural language processing tasks. \n",
      "It involves converting text into a format that is easy for machines to understand. \n",
      "Key steps include stemming, lemmatization, and tokenization.\n",
      "\n",
      "\n",
      "Stemmed Words: ['text', 'preprocess', 'is', 'a', 'critic', 'step', 'in', 'natur', 'languag', 'process', 'task', '.', 'it', 'involv', 'convert', 'text', 'into', 'a', 'format', 'that', 'is', 'easi', 'for', 'machin', 'to', 'understand', '.', 'key', 'step', 'includ', 'stem', ',', 'lemmat', ',', 'and', 'token', '.']\n",
      "\n",
      "NLTK Lemmatized Words: ['Text', 'preprocessing', 'is', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'task', '.', 'It', 'involves', 'converting', 'text', 'into', 'a', 'format', 'that', 'is', 'easy', 'for', 'machine', 'to', 'understand', '.', 'Key', 'step', 'include', 'stemming', ',', 'lemmatization', ',', 'and', 'tokenization', '.']\n",
      "\n",
      "SpaCy Lemmatized Words: ['\\n', 'text', 'preprocessing', 'be', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'task', '.', '\\n', 'it', 'involve', 'convert', 'text', 'into', 'a', 'format', 'that', 'be', 'easy', 'for', 'machine', 'to', 'understand', '.', '\\n', 'key', 'step', 'include', 'stem', ',', 'lemmatization', ',', 'and', 'tokenization', '.', '\\n']\n",
      "\n",
      "NLTK Tokenized Words: ['Text', 'preprocessing', 'is', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'tasks', '.', 'It', 'involves', 'converting', 'text', 'into', 'a', 'format', 'that', 'is', 'easy', 'for', 'machines', 'to', 'understand', '.', 'Key', 'steps', 'include', 'stemming', ',', 'lemmatization', ',', 'and', 'tokenization', '.']\n",
      "\n",
      "SpaCy Tokenized Words: ['\\n', 'Text', 'preprocessing', 'is', 'a', 'critical', 'step', 'in', 'natural', 'language', 'processing', 'tasks', '.', '\\n', 'It', 'involves', 'converting', 'text', 'into', 'a', 'format', 'that', 'is', 'easy', 'for', 'machines', 'to', 'understand', '.', '\\n', 'Key', 'steps', 'include', 'stemming', ',', 'lemmatization', ',', 'and', 'tokenization', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Compare Results\n",
    "print(\"\\nComparison of Techniques:\\n\")\n",
    "print(\"Raw Text:\", raw_text)\n",
    "print(\"\\nStemmed Words:\", stemmed_words)\n",
    "print(\"\\nNLTK Lemmatized Words:\", nltk_lemmatized)\n",
    "print(\"\\nSpaCy Lemmatized Words:\", spacy_lemmatized)\n",
    "print(\"\\nNLTK Tokenized Words:\", nltk_tokens)\n",
    "print(\"\\nSpaCy Tokenized Words:\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. What differences did you observe between stemming and lemmatization?\n",
    "2. Which tokenization technique (NLTK vs. SpaCy) provided better results for your text?\n",
    "3. How might the choice of preprocessing technique impact downstream NLP tasks like classification or summarization?\n",
    "4. Experiment with different texts. How do the results vary for complex sentences or domain-specific text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs Lemmatization\n",
    "\n",
    "**Stemming** and **lemmatization** are both text normalization techniques used in **Natural Language Processing (NLP)** to reduce words to their base or root form. Here's a concise comparison:\n",
    "\n",
    "### Stemming:\n",
    "- **Definition**: Stemming is the process of removing prefixes or suffixes from a word to reduce it to its root form, which may not always be a real word.\n",
    "- **Example**: \n",
    "  - \"running\" → \"run\"\n",
    "  - \"happily\" → \"happi\"\n",
    "- **Approach**: Rule-based, often aggressive (may not result in a valid word).\n",
    "- **Use Case**: Quick and efficient when exact meaning is less important.\n",
    "\n",
    "### Lemmatization:\n",
    "- **Definition**: Lemmatization reduces a word to its base or dictionary form (lemma), ensuring the result is a valid word, often considering the word's part of speech.\n",
    "- **Example**: \n",
    "  - \"running\" → \"run\"\n",
    "  - \"better\" → \"good\" (as an adjective)\n",
    "- **Approach**: Dictionary-based, more precise, takes into account context.\n",
    "- **Use Case**: Preferred when accurate meaning and valid words are necessary.\n",
    "\n",
    "### Summary:\n",
    "- **Stemming** is faster but less accurate and can result in non-existent words.\n",
    "- **Lemmatization** is more accurate, ensuring valid words, but is computationally slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Explanation of the Text Preprocessing Techniques\n",
    "\n",
    "The provided text compares **stemming**, **lemmatization**, and **tokenization** using **NLTK** and **SpaCy**, two popular NLP libraries.\n",
    "\n",
    "### Raw Text:\n",
    "The raw text discusses **text preprocessing**, which involves transforming text into a format suitable for machine understanding. It mentions key preprocessing steps like **stemming**, **lemmatization**, and **tokenization**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Stemmed Words**:\n",
    "- **Stemming** involves reducing words to their root form by removing prefixes or suffixes.\n",
    "- **Example**: \n",
    "  - \"preprocessing\" → \"preprocess\"\n",
    "  - \"critical\" → \"critic\"\n",
    "  - \"language\" → \"languag\"\n",
    "- **Note**: Some stemmed words are not valid dictionary words (e.g., \"critic\" instead of \"critical\").\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **NLTK Lemmatized Words**:\n",
    "- **Lemmatization** reduces words to their base or dictionary form, ensuring they are valid words.\n",
    "- **Example**: \n",
    "  - \"preprocessing\" → \"preprocessing\" (no change)\n",
    "  - \"critical\" → \"critical\"\n",
    "  - \"language\" → \"language\"\n",
    "- **Note**: NLTK lemmatization correctly converts \"converting\" to \"convert\", maintaining valid words.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **SpaCy Lemmatized Words**:\n",
    "- **SpaCy** also performs lemmatization but with slightly different results. For instance:\n",
    "  - \"preprocessing\" stays as \"preprocessing\".\n",
    "  - \"critical\" stays as \"critical\".\n",
    "  - Notice the word \"**be**\" instead of \"is\" in some cases (e.g., \"be easy\" instead of \"is easy\").\n",
    "  - SpaCy also includes `\\n` (newline) tokens, which might indicate unwanted text artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **NLTK Tokenized Words**:\n",
    "- **Tokenization** splits the text into smaller units (tokens) such as words and punctuation marks.\n",
    "- The NLTK tokenizer has correctly separated the words and punctuation: \n",
    "  - `['Text', 'preprocessing', 'is', 'a', ...]`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **SpaCy Tokenized Words**:\n",
    "- **SpaCy Tokenizer** also splits the text into tokens, but includes unwanted `\\n` (newline) characters.\n",
    "- The output of SpaCy is similar to NLTK’s but with additional newline tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Key Differences:\n",
    "- **Stemming** is fast but less accurate, producing root forms that might not be valid words.\n",
    "- **Lemmatization** is more accurate, ensuring valid words, but with slight variations between NLTK and SpaCy.\n",
    "- **Tokenization** splits text into words and punctuation, with both NLTK and SpaCy giving similar results, although SpaCy includes extra newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
