{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Week 1: NLP Pipeline Implementation Project\n",
    "\n",
    "In this project, you will implement a complete text preprocessing pipeline for Natural Language Processing (NLP). The goal is to clean, preprocess, and prepare raw text data for further analysis or modeling. By the end of this project, you will:\n",
    "\n",
    "1. Understand the flow of an NLP pipeline.\n",
    "2. Perform tokenization, lowercasing, punctuation removal, and stopword removal.\n",
    "3. Generate meaningful insights by comparing raw vs. processed text.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem Statement:\n",
    "\n",
    "Given a raw text dataset, implement a preprocessing pipeline that:\n",
    "- Cleans the text data.\n",
    "- Tokenizes sentences and words.\n",
    "- Removes unnecessary characters and stopwords.\n",
    "- Outputs a cleaned version of the text for further use.\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset:\n",
    "You will use a raw text paragraph for this project. Feel free to replace the sample text with your own data or a dataset from a text corpus like NLTK's Gutenberg or Reuters corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download Necessary NLTK Data Files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set Stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Input Text\n",
    "You can use the sample text below or replace it with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Raw Text\n",
    "raw_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field of artificial intelligence. \n",
    "It helps computers understand, interpret, and generate human language. The applications are vast: \n",
    "chatbots, translation, text summarization, and many more. Preprocessing is the first step in any NLP pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Print Raw Text\n",
    "print(\"Raw Text:\\n\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization\n",
    "Tokenize the text into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(raw_text)\n",
    "print(\"\\nSentence Tokenization:\\n\")\n",
    "print(sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(raw_text)\n",
    "print(\"\\nWord Tokenization:\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Lowercasing and Removing Punctuation\n",
    "Normalize the text by converting it to lowercase and removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Lowercase\n",
    "words_lower = [word.lower() for word in words]\n",
    "print(\"\\nLowercased Words:\\n\")\n",
    "print(words_lower)\n",
    "\n",
    "# Remove Punctuation\n",
    "words_no_punct = [word for word in words_lower if word not in string.punctuation]\n",
    "print(\"\\nWords Without Punctuation:\\n\")\n",
    "print(words_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stopword Removal\n",
    "Remove common words (stopwords) that don't carry significant meaning in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "filtered_words = [word for word in words_no_punct if word not in stop_words]\n",
    "print(\"\\nWords After Stopword Removal:\\n\")\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Raw vs. Processed Text\n",
    "Combine the cleaned words into a single string to compare the raw and processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Words to Form Processed Text\n",
    "processed_text = ' '.join(filtered_words)\n",
    "print(\"\\nProcessed Text:\\n\")\n",
    "print(processed_text)\n",
    "\n",
    "# Compare Raw and Processed Text\n",
    "print(\"\\nComparison:\\n\")\n",
    "print(\"Raw Text:\\n\", raw_text)\n",
    "print(\"\\nProcessed Text:\\n\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Word Frequency Distribution\n",
    "Plot the frequency of the most common words in the processed text to understand key themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Word Frequencies\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fdist = FreqDist(filtered_words)\n",
    "\n",
    "# Plot Most Common Words\n",
    "print(\"\\nWord Frequency Distribution:\")\n",
    "fdist.plot(10, title=\"Top 10 Words in Processed Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! ðŸŽ‰\n",
    "You have successfully implemented an end-to-end NLP preprocessing pipeline. Here's a quick recap of what you've achieved:\n",
    "\n",
    "- Tokenized text into sentences and words.\n",
    "- Lowercased the text and removed punctuation.\n",
    "- Filtered out stopwords to retain meaningful words.\n",
    "- Visualized word frequencies to understand key themes in the text.\n",
    "\n",
    "---\n",
    "\n",
    "### Reflection:\n",
    "- What are the limitations of this preprocessing pipeline?\n",
    "- How might preprocessing differ for specific NLP tasks like sentiment analysis vs. text summarization?\n",
    "- Experiment with different texts. How does the processed output vary?\n",
    "\n",
    "Feel free to expand this pipeline with additional techniques like stemming, lemmatization, or domain-specific cleaning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}